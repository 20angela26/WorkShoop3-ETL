##  *ü§ñ Machine Learning Project: Global Happiness Prediction (2015‚Äì2019)*

This project focuses on the development of a Machine Learning model based on a multivariable linear statistical approach to predict the happiness level of different countries worldwide. The dataset comes from the World Happiness Report, supported by the Sustainable Development Solutions Network (SDSN), and includes information from the years 2015 to 2019.

The main objective of this work is not only to build a predictive model but also to demonstrate how it can be integrated into a real-time data pipeline using Apache Kafka, with results stored in a relational MySQL database. This end-to-end architecture combines data analysis, machine learning, and data engineering into a complete predictive system.

### *üß© Phase 1: Exploratory Data Analysis (EDA)*

The first stage of the project consisted of performing an in-depth Exploratory Data Analysis (EDA) to understand the structure, quality, and distribution of the data across the five datasets.

During this process, the data was cleaned, standardized, and unified to ensure consistency across all years. Null and duplicated values were removed, inconsistencies were corrected, and column names were harmonized to maintain a coherent structure. This preprocessing ensured data integrity and prevented downstream errors during model training.

Descriptive statistics and visualization techniques were applied to explore relationships among key variables such as GDP per capita, social support, life expectancy, freedom to make life choices, generosity, and corruption perception. This exploratory step helped identify the features most influential in determining the overall happiness score of each country.

The output of this phase is documented in a dedicated EDA file containing the data analysis, feature correlations, and insights that guided the model training process.

### *üß† Phase 2: Multivariable Linear Model Training*

After data exploration and cleaning, a multivariable linear statistical model was developed using the statsmodels library. The goal of this model was to estimate the happiness score as a function of several explanatory variables.

The dataset was divided into training and test subsets using train_test_split from the scikit-learn library to assess the model‚Äôs generalization capacity. Key performance metrics such as the Coefficient of Determination (R¬≤), Mean Absolute Error (MAE), and Mean Squared Error (MSE) were used to evaluate accuracy and identify possible improvements.

Once the desired accuracy and statistical significance were achieved, the model was serialized with pickle, allowing it to be reused in later phases without retraining. The resulting model successfully captured consistent relationships between socioeconomic and well-being indicators, confirming that GDP, health, and social factors play major roles in happiness prediction.

### *‚öôÔ∏è Phase 3: Real-Time Data Pipeline Simulation with Apache Kafka*

The third phase implemented a real-time data streaming simulation using Apache Kafka. This component connected the trained model to a distributed messaging system to emulate production-like data flow conditions.

The architecture is composed of two main elements: a Producer and a Consumer.

The Producer reads and processes the five CSV files (2015‚Äì2019), performs data cleaning and renaming of key features, and sends the formatted data in JSON format to a Kafka topic. This simulates continuous data ingestion from live sources such as APIs or public databases.

The Consumer subscribes to the Kafka topic, retrieves data streams in real time, loads the pre-trained model, and uses it to predict happiness scores for each country as the data arrives. It then stores both the input data and predicted outputs into a MySQL relational database, creating a structured repository for subsequent analysis.

This simulation demonstrates how a trained machine learning model can be integrated into a real-time prediction pipeline, bridging the gap between statistical modeling and live data processing systems.

###* üßÆ Phase 4: Data Storage and Management with MySQL*

The MySQL database serves as the persistence layer of the pipeline. It stores all processed records and predicted values generated by the consumer.

Each entry in the database includes the original variables and their predicted happiness score, allowing historical tracking, validation, and visualization.

The connection and data insertion process were implemented using the mysql.connector library, ensuring reliable transactions and efficient communication between the Python application and the MySQL server.

üî¨ Phase 5: Results and Conclusions

The multivariable linear model successfully identified significant relationships between social, economic, and health-related variables and national happiness levels.

The complete pipeline demonstrated the feasibility of integrating predictive models into real-time data environments, enabling continuous predictions and potential use in monitoring dashboards or automated decision-making systems.

The next steps include enhancing model performance through regularization techniques such as Ridge or Lasso regression, developing an interactive dashboard using frameworks like Streamlit or Dash, and containerizing the entire system with Docker and Kubernetes for production-level scalability.

This project stands as a practical example of how data science, machine learning, and data engineering can be unified to address global socio-economic challenges with analytical precision and technical rigor.

### *üíª Environment Setup and Library Installation*

To run the project successfully, ensure you have Python 3.8 or higher installed. It is strongly recommended to create a virtual environment before installing dependencies.

To install all required libraries, execute the following command:

pip install pandas numpy statsmodels scikit-learn kafka-python mysql-connector-python jsonpickle


You must also have Apache Kafka and MySQL Server properly configured on your system.
For Linux-based systems, you can install and start Kafka with the following commands:

    sudo apt-get update
    sudo apt-get install default-jdk
    wget https://archive.apache.org/dist/kafka/3.3.1/kafka_2.13-3.3.1.tgz
    tar -xzf kafka_2.13-3.3.1.tgz
    cd kafka_2.13-3.3.1
    bin/zookeeper-server-start.sh config/zookeeper.properties
    bin/kafka-server-start.sh config/server.properties


Once Kafka is running, create the appropriate topics and configure the producer and consumer scripts to match your local setup. Ensure that MySQL is active and accessible with the correct credentials.

### *üß† Python Libraries Used*

The following Python libraries were utilized throughout the project:

    import pandas as pd
    import numpy as np
    import json
    import time
    import os
    import pickle
    from kafka import KafkaProducer, KafkaConsumer
    import statsmodels.api as sm
    import statsmodels.formula.api as smf
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
    import mysql.connector
    

Each library plays a crucial role within the system: pandas and numpy handle data manipulation, statsmodels and scikit-learn provide statistical modeling and evaluation, kafka-python manages the streaming communication, pickle handles model persistence, and mysql.connector manages database transactions.
